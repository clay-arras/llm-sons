{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "747850c4-3b36-4e01-a22e-860114fdb0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_str = '''\n",
    "- keeping the same epochs + parameters and optimizing for loss average across training and validation set. i.e. adding new features (like transitioning from naive bigram model to self attention head and seeing the improvement in loss)\n",
    "- optimizing for time per epoch, i.e. making the training faster while trying to keep the validation/training loss around the same.\n",
    "- also: defining SMALL functions with specific requirements, and testing using examples\n",
    "- pattern: defining smallest reproducible example\n",
    "\n",
    "ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software \n",
    "The Unicode Codespace Letâ€™s start with some general orientation. The basic elements of Unicodeâ€”its â€œcharactersâ€, although that term isnâ€™t quite rightâ€”are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix â€œU+â€, such as U+0041 â€œAâ€ latin capital letter a or U+03B8 â€œÎ¸â€ greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. \n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a095abd-47d8-46d4-88ac-25c4a309298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_str = '''\n",
    "step 1: load and downsample from fineweb\n",
    "step 2: decide between bpe regex and architectures by using experiments (i.e. vocab size/merge count, regex matches, etc.)\n",
    "step 3: optimize current BPE implementation. have a validation set and compare working vs non-working\n",
    "\n",
    "goals: \n",
    "- use gelu, add layer norm at the end\n",
    "- use kroplos vs capybara scaling laws???\n",
    "- optimize the training runs\n",
    "\n",
    "- goal: 200M parameter model, 4B token count\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e396882-8e33-45c9-8854-afd57df51ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlin06/.conda/envs/base.3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "ds = load_dataset(\"parquet\", data_files=\"/scratch/st-sielmann-1/nlin06/fineweb/sample/10BT/*.parquet\", split=\"train\", cache_dir=\"/scratch/st-sielmann-1/nlin06/hf_datasets_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53bf58e2-2e86-4a91-8e08-a0001748fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds['text']\n",
    "\n",
    "# 10b tokens, 14m rows\n",
    "# planning on training on MAXIMUM 5B tokens\n",
    "# maybe do 750m tokens\n",
    "\n",
    "# 0.5 * 0.15\n",
    "# 14868862\n",
    "# sample 0.075 indices from total text, train the bpe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2565e68-9662-4dd7-a961-4d44e667ffbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# '''\n",
    "# improvements:\n",
    "# need to store which merges take place -- done\n",
    "# now: need 50k merges. use tqdm. \n",
    "# 50257\n",
    "# '''\n",
    "# from collections import defaultdict\n",
    "# import pickle\n",
    "\n",
    "# def byte_pair_enc(x, num_merges=100):\n",
    "#     enc_x = list(x.encode(\"utf-8\"))\n",
    "\n",
    "#     decoder_dict = {i: [i] for i in range(256)}\n",
    "#     encoder_dict = {}\n",
    "#     start_idx = 256\n",
    "#     for it in range(num_merges): \n",
    "#         new_idx = it + start_idx\n",
    "#         freqs = defaultdict(lambda: 0)\n",
    "#         for f,s in zip(enc_x, enc_x[1:]):\n",
    "#             freqs[(f, s)] += 1\n",
    "        \n",
    "#         most_freq = max(freqs, key=freqs.get)\n",
    "#         if freqs[most_freq] <= 1:\n",
    "#             break\n",
    "        \n",
    "#         decoder_dict[new_idx] = decoder_dict[most_freq[0]] + decoder_dict[most_freq[1]]\n",
    "#         encoder_dict[most_freq] = new_idx\n",
    "        \n",
    "#         nenc_x = []\n",
    "#         i = 0\n",
    "#         while i < len(enc_x):\n",
    "#             if tuple(enc_x[i:i+2]) == most_freq:\n",
    "#                 nenc_x.append(new_idx)\n",
    "#                 i += 1\n",
    "#             else: \n",
    "#                 nenc_x.append(enc_x[i])\n",
    "#             i += 1\n",
    "#         enc_x = nenc_x\n",
    "\n",
    "#         if it % 10 == 0: \n",
    "#             with open(f'ckpt/bpe/enc_{it}.pkl', 'wb') as f:\n",
    "#                 pickle.dump(encoder_dict, f)\n",
    "#             with open(f'ckpt/bpe/dec_{it}.pkl', 'wb') as f:\n",
    "#                 pickle.dump(decoder_dict, f)\n",
    "\n",
    "#     return encoder_dict, decoder_dict\n",
    "\n",
    "\n",
    "# enc, dec = byte_pair_enc(arb_str)\n",
    "# enc = encoder(enc)\n",
    "# dec = decoder(dec)\n",
    "\n",
    "# assert arb_str == dec(enc(arb_str)), dec(enc)\n",
    "# print(\"Success!\")\n",
    "\n",
    "\n",
    "def byte_pair_enc(xs, num_merges): # xs is a list of strings\n",
    "    enc_xs = [list(x.encode(\"utf-8\")) for x in xs]\n",
    "\n",
    "    decoder_dict = {i: [i] for i in range(256)}\n",
    "    encoder_dict = {}\n",
    "    start_idx = 256\n",
    "    for it in tqdm(range(num_merges)): \n",
    "        new_idx = it + start_idx\n",
    "        freqs = defaultdict(lambda: 0)\n",
    "        for enc_x in enc_xs:\n",
    "            for f,s in zip(enc_x, enc_x[1:]):\n",
    "                freqs[(f, s)] += 1\n",
    "        \n",
    "        most_freq = max(freqs, key=freqs.get)\n",
    "        if freqs[most_freq] <= 1:\n",
    "            break\n",
    "        \n",
    "        decoder_dict[new_idx] = decoder_dict[most_freq[0]] + decoder_dict[most_freq[1]]\n",
    "        encoder_dict[most_freq] = new_idx\n",
    "\n",
    "        for xs_idx in range(len(enc_xs)):\n",
    "            enc_x = enc_xs[xs_idx]\n",
    "            nenc_x = []\n",
    "            i = 0\n",
    "            while i < len(enc_x):\n",
    "                if tuple(enc_x[i:i+2]) == most_freq:\n",
    "                    nenc_x.append(new_idx)\n",
    "                    i += 1\n",
    "                else: \n",
    "                    nenc_x.append(enc_x[i])\n",
    "                i += 1\n",
    "            enc_xs[xs_idx] = nenc_x\n",
    "\n",
    "        if it % 10 == 0: \n",
    "            with open(f'ckpt/bpe/enc_{it}.pkl', 'wb') as f:\n",
    "                pickle.dump(encoder_dict, f)\n",
    "            with open(f'ckpt/bpe/dec_{it}.pkl', 'wb') as f:\n",
    "                pickle.dump(decoder_dict, f)\n",
    "\n",
    "    return encoder_dict, decoder_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71244c54-aa07-4e92-a77e-e6cd5fcf4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encoder(encoder_dict):\n",
    "    def wrapper(x):\n",
    "        x = list(x.encode(\"utf-8\"))\n",
    "        enc_x = x\n",
    "        for k, v in encoder_dict.items():\n",
    "            nenc_x = []\n",
    "            i = 0\n",
    "            while i < len(enc_x):\n",
    "                if tuple(enc_x[i:i+2]) == k:\n",
    "                    nenc_x.append(v)\n",
    "                    i += 1\n",
    "                else: \n",
    "                    nenc_x.append(enc_x[i])\n",
    "                i += 1\n",
    "            enc_x = nenc_x\n",
    "        return enc_x\n",
    "    return wrapper\n",
    "    \n",
    "def decoder(decoder_dict): \n",
    "    def wrapper(x):\n",
    "        dec_x = [] \n",
    "        for i in x:\n",
    "            dec_x.extend(decoder_dict[i])\n",
    "        dec_x = bytes(dec_x).decode(\"utf-8\", errors=\"replace\")\n",
    "        return dec_x\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b468346-2230-436f-bb45-2440df6e4b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('ckpt/enc.pkl', 'rb') as f:\n",
    "    enc = pickle.load(f)\n",
    "with open('ckpt/dec.pkl', 'rb') as f:\n",
    "    dec = pickle.load(f)\n",
    "\n",
    "enc = encoder(enc)\n",
    "dec = decoder(dec)\n",
    "\n",
    "assert arb_str == dec(enc(arb_str)), dec(enc)\n",
    "print(\"Success!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dd05ca-7daf-481a-9cbf-038159da90b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "now: need to \n",
    "a) sample from dataset a percentage x times\n",
    "b) now we have k text pieces in an array. then, we split them according to regex, so we have a longer array of strings of \"important features\"\n",
    "c) then, we run BPE on it.\n",
    "\n",
    "next step: modify bpe s.t. it takes in a LIST of x's representing strings. \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dae5335f-5f2e-481d-a2eb-714605652198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "# def byte_pair_enc(xs, num_merges=100): # xs is a list of strings\n",
    "#     enc_xs = [list(x.encode(\"utf-8\")) for x in xs]\n",
    "\n",
    "#     decoder_dict = {i: [i] for i in range(256)}\n",
    "#     encoder_dict = {}\n",
    "#     start_idx = 256\n",
    "#     for it in range(num_merges): \n",
    "#         new_idx = it + start_idx\n",
    "#         freqs = defaultdict(lambda: 0)\n",
    "#         for enc_x in enc_xs:\n",
    "#             for f,s in zip(enc_x, enc_x[1:]):\n",
    "#                 freqs[(f, s)] += 1\n",
    "        \n",
    "#         most_freq = max(freqs, key=freqs.get)\n",
    "#         if freqs[most_freq] <= 1:\n",
    "#             break\n",
    "        \n",
    "#         decoder_dict[new_idx] = decoder_dict[most_freq[0]] + decoder_dict[most_freq[1]]\n",
    "#         encoder_dict[most_freq] = new_idx\n",
    "\n",
    "#         for xs_idx in range(len(enc_xs)):\n",
    "#             enc_x = enc_xs[xs_idx]\n",
    "#             nenc_x = []\n",
    "#             i = 0\n",
    "#             while i < len(enc_x):\n",
    "#                 if tuple(enc_x[i:i+2]) == most_freq:\n",
    "#                     nenc_x.append(new_idx)\n",
    "#                     i += 1\n",
    "#                 else: \n",
    "#                     nenc_x.append(enc_x[i])\n",
    "#                 i += 1\n",
    "#             enc_xs[xs_idx] = nenc_x\n",
    "\n",
    "#         if it % 10 == 0: \n",
    "#             with open(f'ckpt/bpe/enc_{it}.pkl', 'wb') as f:\n",
    "#                 pickle.dump(encoder_dict, f)\n",
    "#             with open(f'ckpt/bpe/dec_{it}.pkl', 'wb') as f:\n",
    "#                 pickle.dump(decoder_dict, f)\n",
    "\n",
    "#     return encoder_dict, decoder_dict\n",
    "\n",
    "\n",
    "enc, dec = byte_pair_enc_ls([arb_str, gl_str, \"hello world\", \"this is a random string\"])\n",
    "enc = encoder(enc)\n",
    "dec = decoder(dec)\n",
    "\n",
    "print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb7003fa-0180-43a0-b460-87078e761409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlin06/.conda/envs/base.3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "ds = load_dataset(\"parquet\", data_files=\"/scratch/st-sielmann-1/nlin06/fineweb/sample/10BT/*.parquet\", split=\"train\", cache_dir=\"/scratch/st-sielmann-1/nlin06/hf_datasets_cache\")\n",
    "\n",
    "# ds['text']\n",
    "\n",
    "# 10b tokens, 14m rows\n",
    "# planning on training on MAXIMUM 5B tokens\n",
    "# maybe do 750m tokens\n",
    "\n",
    "# 0.5 * 0.15\n",
    "# 14868862\n",
    "# sample 0.075 indices from total text, train the bpe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42b67eb5-508a-4e43-ba9c-d80225803fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115164"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(0.5 * 0.15* len(ds['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3057e088-82aa-493a-8488-22f9a8b31470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# N = len(ds['text'])\n",
    "\n",
    "# torch.multinomial(N, 10, replacement=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98c4f8b-5dc2-4e30-aaba-09b6bb993c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11017429,  8145658, 11895971,  ...,  1006107, 12313928, 14854839])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(ds['text'])\n",
    "sub_N = int(0.5 * 0.15 * len(ds['text'])\n",
    "\n",
    "torch.randperm(N)[:sub_N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ef1ac2e-f0ac-42f4-87a7-f1c7d53f54aa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandperm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(i)\n",
      "\u001b[31mTypeError\u001b[39m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "for i in range(torch.randperm(N)[:4]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042ce13-db52-49a1-ac35-2f52d9024823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base.3.11",
   "language": "python",
   "name": "base.3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
