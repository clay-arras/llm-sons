{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "747850c4-3b36-4e01-a22e-860114fdb0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_str = '''\n",
    "- keeping the same epochs + parameters and optimizing for loss average across training and validation set. i.e. adding new features (like transitioning from naive bigram model to self attention head and seeing the improvement in loss)\n",
    "- optimizing for time per epoch, i.e. making the training faster while trying to keep the validation/training loss around the same.\n",
    "- also: defining SMALL functions with specific requirements, and testing using examples\n",
    "- pattern: defining smallest reproducible example\n",
    "\n",
    "Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software \n",
    "The Unicode Codespace Let’s start with some general orientation. The basic elements of Unicode—its “characters”, although that term isn’t quite right—are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix “U+”, such as U+0041 “A” latin capital letter a or U+03B8 “θ” greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. \n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0cc2a62b-93fb-4184-bf52-9e308ec3e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INCREDIBLE! HOMEMADE GARBAGE!! :))))\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def byte_pair_enc(x, vocab_size = 276):\n",
    "    enc_x = list(x.encode(\"utf-8\"))\n",
    "    vocab_list = sorted(list(set(enc_x))) # original\n",
    "    # to test: need to encode the string, check if its shorter, then decode\n",
    "    \n",
    "    new_tokens = {}\n",
    "    start_idx = 256\n",
    "    num_merges = vocab_size - 256\n",
    "    while True: \n",
    "        freqs = defaultdict(lambda: 0)\n",
    "        for i in range(len(enc_x)-1):\n",
    "            freqs[tuple(enc_x[i:i+2])] += 1\n",
    "        # for i in range(zip(enc_x, enc_x[1:]))\n",
    "        \n",
    "        most_freq = max(freqs, key=freqs.get)\n",
    "        if freqs[most_freq] <= 1 or num_merges < 0:\n",
    "            break\n",
    "        \n",
    "        new_tokens[start_idx] = most_freq\n",
    "        nenc_x = []\n",
    "        i = len(enc_x)-1\n",
    "        while i >= 0:\n",
    "            if tuple(enc_x[i-1:i+1]) == most_freq:\n",
    "                nenc_x.append(start_idx)\n",
    "                i -= 1\n",
    "            else: \n",
    "                nenc_x.append(enc_x[i])\n",
    "            i -= 1\n",
    "        enc_x = nenc_x[::-1]\n",
    "        \n",
    "        start_idx += 1\n",
    "        num_merges -= 1\n",
    "\n",
    "    def decoder(x): \n",
    "        while max(x) >= 256: # we can cache these using dp, but too lazy\n",
    "            dec_x = [] \n",
    "            for i in x:\n",
    "                if i >= 256:\n",
    "                    dec_x.extend(list(new_tokens[i]))\n",
    "                else:\n",
    "                    dec_x.append(i)\n",
    "            x = dec_x\n",
    "        return bytes(dec_x).decode(\"utf-8\", errors=\"replace\")\n",
    "        \n",
    "    return enc_x, decoder\n",
    "\n",
    "enc, dec = byte_pair_enc(arb_str)\n",
    "assert arb_str == dec(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a095abd-47d8-46d4-88ac-25c4a309298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gl_str = '''\n",
    "step 1: load and downsample from fineweb\n",
    "step 2: decide between bpe regex and architectures by using experiments (i.e. vocab size/merge count, regex matches, etc.)\n",
    "step 3: optimize current BPE implementation. have a validation set and compare working vs non-working\n",
    "\n",
    "goals: \n",
    "- use gelu, add layer norm at the end\n",
    "- use kroplos vs capybara scaling laws???\n",
    "- optimize the training runs\n",
    "\n",
    "- goal: 200M parameter model, 4B token count\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e396882-8e33-45c9-8854-afd57df51ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlin06/.conda/envs/base.3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "ds = load_dataset(\"parquet\", data_files=\"/scratch/st-sielmann-1/nlin06/fineweb/sample/10BT/*.parquet\", split=\"train\", cache_dir=\"/scratch/st-sielmann-1/nlin06/hf_datasets_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696c9bac-5fb5-44b5-b9fb-4f75be2d13f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'id', 'dump', 'url', 'date', 'file_path', 'language', 'language_score', 'token_count'],\n",
       "    num_rows: 14868862\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53bf58e2-2e86-4a91-8e08-a0001748fd4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.075"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['text']\n",
    "\n",
    "# 10b tokens, 14m rows\n",
    "# planning on training on MAXIMUM 5B tokens\n",
    "# maybe do 750m tokens\n",
    "\n",
    "# 0.5 * 0.15\n",
    "# 14868862\n",
    "# sample 0.075 indices from total text, train the bpe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2565e68-9662-4dd7-a961-4d44e667ffbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "improvements:\n",
    "need to store which merges take place\n",
    "'''\n",
    "\n",
    "import pickle\n",
    "\n",
    "def byte_pair_enc(x, num_merges=100):\n",
    "    enc_x = list(x.encode(\"utf-8\"))\n",
    "\n",
    "    decoder_dict = {i: [i] for i in range(256)}\n",
    "    encoder_dict = {}\n",
    "    start_idx = 256\n",
    "    for new_idx in range(start_idx, start_idx + num_merges): \n",
    "        freqs = defaultdict(lambda: 0)\n",
    "        for f,s in zip(enc_x, enc_x[1:]):\n",
    "            freqs[(f, s)] += 1\n",
    "        \n",
    "        most_freq = max(freqs, key=freqs.get)\n",
    "        if freqs[most_freq] <= 1:\n",
    "            break\n",
    "        \n",
    "        decoder_dict[new_idx] = decoder_dict[most_freq[0]] + decoder_dict[most_freq[1]]\n",
    "        encoder_dict[most_freq] = new_idx\n",
    "        \n",
    "        nenc_x = []\n",
    "        i = 0\n",
    "        while i < len(enc_x):\n",
    "            if tuple(enc_x[i:i+2]) == most_freq:\n",
    "                nenc_x.append(new_idx)\n",
    "                i += 1\n",
    "            else: \n",
    "                nenc_x.append(enc_x[i])\n",
    "            i += 1\n",
    "        enc_x = nenc_x\n",
    "\n",
    "    return encoder_dict, decoder_dict\n",
    "\n",
    "\n",
    "enc, dec = byte_pair_enc(arb_str)\n",
    "\n",
    "with open('ckpt/enc.pkl', 'wb') as f:\n",
    "    pickle.dump(enc, f)\n",
    "with open('ckpt/dec.pkl', 'wb') as f:\n",
    "    pickle.dump(dec, f)\n",
    "\n",
    "\n",
    "enc = encoder(enc)\n",
    "dec = decoder(dec)\n",
    "\n",
    "assert arb_str == dec(enc(arb_str)), dec(enc)\n",
    "print(\"Success!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71244c54-aa07-4e92-a77e-e6cd5fcf4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def encoder(encoder_dict):\n",
    "    def wrapper(x):\n",
    "        x = list(x.encode(\"utf-8\"))\n",
    "        enc_x = x\n",
    "        for k, v in encoder_dict.items():\n",
    "            nenc_x = []\n",
    "            i = 0\n",
    "            while i < len(enc_x):\n",
    "                if tuple(enc_x[i:i+2]) == k:\n",
    "                    nenc_x.append(v)\n",
    "                    i += 1\n",
    "                else: \n",
    "                    nenc_x.append(enc_x[i])\n",
    "                i += 1\n",
    "            enc_x = nenc_x\n",
    "        return enc_x\n",
    "    return wrapper\n",
    "    \n",
    "def decoder(decoder_dict): \n",
    "    def wrapper(x):\n",
    "        dec_x = [] \n",
    "        for i in x:\n",
    "            dec_x.extend(decoder_dict[i])\n",
    "        dec_x = bytes(dec_x).decode(\"utf-8\", errors=\"replace\")\n",
    "        return dec_x\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b468346-2230-436f-bb45-2440df6e4b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('ckpt/enc.pkl', 'rb') as f:\n",
    "    enc = pickle.load(f)\n",
    "with open('ckpt/dec.pkl', 'rb') as f:\n",
    "    dec = pickle.load(f)\n",
    "\n",
    "enc = encoder(enc)\n",
    "dec = decoder(dec)\n",
    "\n",
    "assert arb_str == dec(enc(arb_str)), dec(enc)\n",
    "print(\"Success!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dd05ca-7daf-481a-9cbf-038159da90b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base.3.11",
   "language": "python",
   "name": "base.3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
