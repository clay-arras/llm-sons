{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49e50530-3d42-4f52-bd43-2bd30b000424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a100d57-9fa9-4185-b406-797abcc010de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/input.txt\", \"r\") as f:\n",
    "  text = f.read()\n",
    "\n",
    "vocab_list = ''.join(sorted(list(set(text))))\n",
    "vocab_size = len(vocab_list)\n",
    "\n",
    "ctoi = { val: i for i, val in enumerate(vocab_list)}\n",
    "itoc = { i: val for i, val in enumerate(vocab_list)}\n",
    "encoder = lambda s: [ctoi[i] for i in s]\n",
    "decoder = lambda enc: ''.join([itoc[i] for i in enc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ecf8324-a414-487e-b3b7-04cb7ddd3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoder(text)\n",
    "tt_split = 0.9\n",
    "\n",
    "train = torch.tensor(enc[int(tt_split*len(enc)):])\n",
    "test = torch.tensor(enc[:int(tt_split*len(enc))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bdbc453-0378-47c8-ba4a-4ce39da17166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa74dd1a9f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_size = 384\n",
    "batch_size = 64\n",
    "context_size = 256\n",
    "dropout_thres = 0.25\n",
    "n_heads = 6\n",
    "n_layers = 6\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1adcfb79-7248-44f4-8aa8-ea431ecb9bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data):\n",
    "  '''\n",
    "  in:\n",
    "  - data: tensor (n,)\n",
    "  out:\n",
    "  - x_batch: tensor (batch_size, context_size)\n",
    "  - y_batch: tensor (batch_size, context_size)\n",
    "  '''\n",
    "  pos = torch.randint(0, len(data) - context_size-1, (batch_size,)) # -1 for the ys\n",
    "\n",
    "  x_batch = torch.stack([data[i_pos:i_pos+context_size] for i_pos in pos])\n",
    "  y_batch = torch.stack([data[i_pos+1:i_pos+context_size+1] for i_pos in pos])\n",
    "  return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2caed08c-7f8d-417b-96f5-d4ec11996cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(torch.nn.Module):\n",
    "  def __init__(self, head_size):\n",
    "    super().__init__()\n",
    "    self.head_size = head_size\n",
    "    self.key = torch.nn.Linear(embd_size, head_size, bias=False)\n",
    "    self.value = torch.nn.Linear(embd_size, head_size, bias=False)\n",
    "    self.query = torch.nn.Linear(embd_size, head_size, bias=False)\n",
    "\n",
    "    self.drop = torch.nn.Dropout(dropout_thres)\n",
    "    self.register_buffer(\"tril\", torch.tril(torch.ones((context_size, context_size)))) \n",
    "  \n",
    "  def forward(self, x):\n",
    "    B, T, C = x.shape\n",
    "    k = self.key(x) # (B, T, head_size)\n",
    "    v = self.value(x) # (B, T, head_size)\n",
    "    q = self.query(x) # (B, T, head_size)\n",
    "    \n",
    "    W = k @ q.transpose(-1, -2) * self.head_size**(-0.5) # (B, T, T)\n",
    "    W.masked_fill_(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "    w_mask = torch.nn.functional.softmax(W, dim=1) # (B, T, T)\n",
    "    w_mask = self.drop(w_mask)\n",
    "\n",
    "    return w_mask @ v # (B, T, head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7adfef0-bd91-48f9-aabb-dccb1bb8218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionHead(torch.nn.Module):\n",
    "  def __init__(self, n_heads, head_size):\n",
    "    super().__init__()\n",
    "    self.heads = torch.nn.ModuleList(\n",
    "        SelfAttentionHead(head_size) for _ in range(n_heads)\n",
    "    )\n",
    "    self.proj = torch.nn.Linear(n_heads * head_size, embd_size)\n",
    "    self.drop = torch.nn.Dropout(dropout_thres)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    st = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    return self.drop(self.proj(st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f9f130-5e6b-4568-9876-b328b54f61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(torch.nn.Module):\n",
    "  def __init__(self, nin, nout):\n",
    "    super().__init__()\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        torch.nn.Linear(nin, 4*nout),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(4*nout, nout),\n",
    "        torch.nn.Dropout(dropout_thres)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e43ef71d-61fb-49a1-b2e5-f15547dbd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.sa_heads = MultiAttentionHead(n_heads, embd_size // n_heads)\n",
    "    self.ffwd = FeedForwardLayer(embd_size, embd_size)\n",
    "\n",
    "    self.ln1 = torch.nn.LayerNorm((embd_size,))\n",
    "    self.ln2 = torch.nn.LayerNorm((embd_size,))\n",
    "  \n",
    "  def forward(self, x): \n",
    "    '''\n",
    "    in: \n",
    "    - x: tensor (batch_size, context_size, embd_size)\n",
    "    out: \n",
    "    - out: tensor (batch_size, context_size, embd_size)\n",
    "    '''\n",
    "    x = self.sa_heads(self.ln1(x)) + x # residual connections\n",
    "    x = self.ffwd(self.ln2(x)) + x\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1f6615d-6a09-4a8c-a36d-747d0cff6142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.feat_encoding = torch.nn.Embedding(vocab_size, embd_size)\n",
    "    self.pos_encoding = torch.nn.Embedding(context_size, embd_size)\n",
    "\n",
    "    self.blocks = torch.nn.Sequential(\n",
    "        *[Block() for _ in range(n_layers)],\n",
    "        torch.nn.Linear(embd_size, vocab_size)\n",
    "    )\n",
    "\n",
    "  def forward(self, xs, ys=None): # outputs logits, loss\n",
    "    '''\n",
    "    in:\n",
    "    - xs: tensor (batch_size, context_size)\n",
    "    - ys: tensor (batch_size, context_size) or None\n",
    "    out:\n",
    "    - logits: tensor (batch_size, context_size, vocab_size)\n",
    "    - loss: tensor (1,) or None\n",
    "    '''\n",
    "    \n",
    "    f = self.feat_encoding(xs) # (batch_size, context_size, embd_size)\n",
    "    p = self.pos_encoding(torch.arange(0, xs.shape[1], device=device)) # (context_size, embd_size)\n",
    "\n",
    "    x = f + p \n",
    "    logits = self.blocks(x)\n",
    "    B, C, V = logits.shape\n",
    "\n",
    "    if ys is not None:\n",
    "      logits_ce = logits.reshape(B*C, V)\n",
    "      ys_ce = ys.reshape(B*C)\n",
    "\n",
    "      loss = torch.nn.functional.cross_entropy(logits_ce, ys_ce)\n",
    "    else:\n",
    "      loss = None\n",
    "    return logits, loss\n",
    "\n",
    "  def generator(self, max_length, batch_size_):\n",
    "    '''\n",
    "    in:\n",
    "    - max_length: int\n",
    "    out:\n",
    "    - out: tensor (batch_size, max_length)\n",
    "    '''\n",
    "    out = torch.zeros((batch_size_, 1), dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(max_length - 1):\n",
    "      last_char = out[:, -context_size:]\n",
    "\n",
    "      logits, _ = self.forward(last_char)\n",
    "\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "      ntoken = torch.multinomial(probs, 1) \n",
    "      out = torch.cat((out, ntoken), dim=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74398d67-f211-464f-b9bb-2d4b40ec847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 87/5000 [00:13<13:08,  6.23it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     31\u001b[39m       optimizer.step()\n\u001b[32m     33\u001b[39m   \u001b[38;5;28mprint\u001b[39m(get_metrics(model))\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# (tensor(1.9297, grad_fn=<MeanBackward0>), tensor(2.2202, grad_fn=<MeanBackward0>))\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtest_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     25\u001b[39m logits, loss = model(x, y)\n\u001b[32m     27\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(): \n\u001b[32m     31\u001b[39m   optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/base.3.11/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    484\u001b[39m         Tensor.backward,\n\u001b[32m    485\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m         inputs=inputs,\n\u001b[32m    491\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/base.3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    246\u001b[39m     retain_graph = create_graph\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_metrics(model, num_samples=10):\n",
    "  model.eval()\n",
    "  train_losses, test_losses = [], []\n",
    "  for _ in range(num_samples):\n",
    "    x, y = get_batch(train)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    train_losses.append(model(x, y)[1].item())\n",
    "\n",
    "    x, y = get_batch(test)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    test_losses.append(model(x, y)[1].item())\n",
    "  model.train()\n",
    "  return torch.tensor(train_losses).mean(), torch.tensor(test_losses).mean()\n",
    "\n",
    "def test_model():\n",
    "  model = BigramLanguageModel().to(device)\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "  for _ in tqdm(range(5000)):\n",
    "    x, y = get_batch(train)\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    logits, loss = model(x, y)\n",
    "  \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "  \n",
    "    with torch.no_grad(): \n",
    "      optimizer.step()\n",
    "  \n",
    "  print(get_metrics(model))\n",
    "\n",
    "test_model()\n",
    "# (tensor(1.9297, grad_fn=<MeanBackward0>), tensor(2.2202, grad_fn=<MeanBackward0>))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "147957e4-6015-47db-bd10-ebdb835eeba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [26:04<00:00,  6.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(0.0071), tensor(0.3266))\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "# if torch.cuda.device_count() > 1:\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "model.to(device) \n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "epochs = 10000\n",
    "# epochs = 10\n",
    "\n",
    "for _ in tqdm(range(epochs)):\n",
    "  x, y = get_batch(train)\n",
    "  x, y = x.to(\"cuda\"), y.to(\"cuda\")\n",
    "  logits, loss = model(x, y)\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    optimizer.step()\n",
    "\n",
    "print(get_metrics(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "849717bd-51d9-4700-806d-5a86f57de579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AE.YZ--ZZZZ;$ 3GZ&Z$3X$o3....XX$33 $a& a$a.'''' bz3X3sssss,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,, oooooooooooooooooooo???\n",
      "\n",
      "Yiallllllll\n",
      "LLLLL?\n",
      "Upi,!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "S!\n",
      "G-DD  w  \n",
      "\n",
      "\n",
      "a  a aa,a a\n",
      "i a\n",
      "'J'' la e\n",
      "an-\n",
      "A'l  a a\n",
      " a a pail, and mat gawated, Signtio?\n",
      "\n",
      "Dir, gag-tut thell, if thie, match wall patitle!\n",
      "\n",
      "HORTENSIO:\n",
      "My sauty jay, at what you king!\n",
      "\n",
      "or seye mitty.\n",
      "Hereg?\n",
      "\n",
      "WR yher weak, and thy noter hall me,\n",
      "My Dare\n",
      "Worce atay, thirg? ul peat iar,\n",
      "zeep haves\n",
      "Wa your thet me woodey the madvere o her yade\n",
      "Sixtaintor muth will of Yey your\n",
      "Yeare up\n",
      "WoxDe-Since o noter,\n",
      "I gignggiong shy here;\n",
      "Wiar, shall very o'll I kinge?\n",
      "\n",
      "ALIAND:\n",
      "Vaal the mack shall all ke matere to ame\n",
      "Shave sot meght mince obee,\n",
      "\n",
      "'Have the mink!\n",
      "\n",
      "GRUMIO:\n",
      "What hedde with day midging caparminon betth.\n",
      "\n",
      "Put that minth his, whe the, my gringion.\n",
      "\n",
      " wouldand sery grogum is ovut your gidilian,\n",
      "Have off orreck, gor.\n",
      "\n",
      "ANTONIO:\n",
      "Have woinca, a ountant:\n",
      "Dow leave your\n",
      " layive madling!\n",
      "What have galov\n"
     ]
    }
   ],
   "source": [
    "print(decoder(model.generator(1000, 1).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d072e2cc-a110-4d76-8404-40e197b86da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"ckpt/model.0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7577cd7-5790-4550-a2d3-acb8af87a8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base.3.11",
   "language": "python",
   "name": "base.3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
