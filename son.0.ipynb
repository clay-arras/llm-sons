{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49e50530-3d42-4f52-bd43-2bd30b000424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps: \n",
    "'''\n",
    "- optimize BEFORE scale up (time per epoch), architecture changes (loss given set amount of epochs)\n",
    "- pretokenization using BPE (make sure to use gpt4 regex magic)\n",
    "- test distributed training arrays using 2 nodes + 1 gpu each\n",
    "- train on array of 4 nodes w/ 4 V100 gpus each for 24 hours\n",
    "'''\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a100d57-9fa9-4185-b406-797abcc010de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/input.txt\", \"r\") as f:\n",
    "  text = f.read()\n",
    "\n",
    "vocab_list = ''.join(sorted(list(set(text))))\n",
    "vocab_size = len(vocab_list)\n",
    "\n",
    "ctoi = { val: i for i, val in enumerate(vocab_list)}\n",
    "itoc = { i: val for i, val in enumerate(vocab_list)}\n",
    "encoder = lambda s: [ctoi[i] for i in s]\n",
    "decoder = lambda enc: ''.join([itoc[i] for i in enc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ecf8324-a414-487e-b3b7-04cb7ddd3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = encoder(text)\n",
    "tt_split = 0.9\n",
    "\n",
    "test = torch.tensor(enc[int(tt_split*len(enc)):])\n",
    "train = torch.tensor(enc[:int(tt_split*len(enc))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bdbc453-0378-47c8-ba4a-4ce39da17166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f80653ce9f0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_size = 384\n",
    "batch_size = 64\n",
    "context_size = 256\n",
    "dropout_thres = 0.2\n",
    "n_heads = 6\n",
    "n_layers = 6\n",
    "learning_rate = 1e-4\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2caed08c-7f8d-417b-96f5-d4ec11996cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionHead(torch.nn.Module):\n",
    "  def __init__(self, head_size):\n",
    "    super().__init__()\n",
    "    self.head_size = head_size\n",
    "    self.key = torch.nn.Linear(embd_size, head_size, bias=False)\n",
    "    self.value = torch.nn.Linear(embd_size, head_size, bias=False)\n",
    "    self.query = torch.nn.Linear(embd_size, head_size, bias=False)\n",
    "\n",
    "    self.drop = torch.nn.Dropout(dropout_thres)\n",
    "    self.register_buffer(\"tril\", torch.tril(torch.ones((context_size, context_size)))) \n",
    "  \n",
    "  def forward(self, x):\n",
    "    B, T, C = x.shape\n",
    "    k = self.key(x) # (B, T, head_size)\n",
    "    v = self.value(x) # (B, T, head_size)\n",
    "    q = self.query(x) # (B, T, head_size)\n",
    "    \n",
    "    W = q @ k.transpose(-1, -2) * self.head_size**(-0.5) # (B, T, T)\n",
    "    W.masked_fill_(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "    w_mask = torch.nn.functional.softmax(W, dim=-1) # (B, T, T)\n",
    "    w_mask = self.drop(w_mask)\n",
    "\n",
    "    return w_mask @ v # (B, T, head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a7adfef0-bd91-48f9-aabb-dccb1bb8218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAttentionHead(torch.nn.Module):\n",
    "  def __init__(self, n_heads, head_size):\n",
    "    super().__init__()\n",
    "    self.heads = torch.nn.ModuleList(\n",
    "        SelfAttentionHead(head_size) for _ in range(n_heads)\n",
    "    )\n",
    "    self.proj = torch.nn.Linear(n_heads * head_size, embd_size)\n",
    "    self.drop = torch.nn.Dropout(dropout_thres)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    st = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    return self.drop(self.proj(st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6f9f130-5e6b-4568-9876-b328b54f61d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(torch.nn.Module):\n",
    "  def __init__(self, nin, nout):\n",
    "    super().__init__()\n",
    "    self.layers = torch.nn.Sequential(\n",
    "        torch.nn.Linear(nin, 4*nout),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(4*nout, nout),\n",
    "        torch.nn.Dropout(dropout_thres)\n",
    "    )\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e43ef71d-61fb-49a1-b2e5-f15547dbd874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.sa_heads = MultiAttentionHead(n_heads, embd_size // n_heads)\n",
    "    self.ffwd = FeedForwardLayer(embd_size, embd_size)\n",
    "\n",
    "    self.ln1 = torch.nn.LayerNorm((embd_size,))\n",
    "    self.ln2 = torch.nn.LayerNorm((embd_size,))\n",
    "  \n",
    "  def forward(self, x): \n",
    "    '''\n",
    "    in: \n",
    "    - x: tensor (batch_size, context_size, embd_size)\n",
    "    out: \n",
    "    - out: tensor (batch_size, context_size, embd_size)\n",
    "    '''\n",
    "    x = self.sa_heads(self.ln1(x)) + x # residual connections\n",
    "    x = self.ffwd(self.ln2(x)) + x\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1f6615d-6a09-4a8c-a36d-747d0cff6142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.feat_encoding = torch.nn.Embedding(vocab_size, embd_size)\n",
    "    self.pos_encoding = torch.nn.Embedding(context_size, embd_size)\n",
    "\n",
    "    self.blocks = torch.nn.Sequential(\n",
    "        *[Block() for _ in range(n_layers)],\n",
    "        torch.nn.LayerNorm((embd_size,)),\n",
    "        torch.nn.Linear(embd_size, vocab_size)\n",
    "    )\n",
    "\n",
    "  def forward(self, xs, ys=None): # outputs logits, loss\n",
    "    '''\n",
    "    in:\n",
    "    - xs: tensor (batch_size, context_size)\n",
    "    - ys: tensor (batch_size, context_size) or None\n",
    "    out:\n",
    "    - logits: tensor (batch_size, context_size, vocab_size)\n",
    "    - loss: tensor (1,) or None\n",
    "    '''\n",
    "    \n",
    "    f = self.feat_encoding(xs) # (batch_size, context_size, embd_size)\n",
    "    p = self.pos_encoding(torch.arange(0, xs.shape[1], device=device)) # (context_size, embd_size)\n",
    "\n",
    "    x = f + p \n",
    "    logits = self.blocks(x)\n",
    "    B, C, V = logits.shape\n",
    "\n",
    "    if ys is not None:\n",
    "      logits_ce = logits.reshape(B*C, V)\n",
    "      ys_ce = ys.reshape(B*C)\n",
    "\n",
    "      loss = torch.nn.functional.cross_entropy(logits_ce, ys_ce)\n",
    "    else:\n",
    "      loss = None\n",
    "    return logits, loss\n",
    "\n",
    "  def generator(self, max_length, batch_size_):\n",
    "    '''\n",
    "    in:\n",
    "    - max_length: int\n",
    "    out:\n",
    "    - out: tensor (batch_size, max_length)\n",
    "    '''\n",
    "    out = torch.zeros((batch_size_, 1), dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(max_length - 1):\n",
    "      last_char = out[:, -context_size:]\n",
    "\n",
    "      logits, _ = self.forward(last_char)\n",
    "\n",
    "      logits = logits[:, -1, :]\n",
    "      probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "      ntoken = torch.multinomial(probs, 1) \n",
    "      out = torch.cat((out, ntoken), dim=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "888e518b-f24e-49a9-9f03-53d9651bfdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer: \n",
    "    def __init__(self, model, train_data, optimizer, gpu_id, save_every): \n",
    "        self.gpu_id = gpu_id\n",
    "        self.model = model.to(gpu_id)\n",
    "        self.train_data = train_data\n",
    "        self.optimizer = optimizer\n",
    "        self.model = DDP(self.model, device_ids=[self.gpu_id])\n",
    "        self.save_every = save_every\n",
    "\n",
    "\n",
    "    def _run_epoch(self, epoch): \n",
    "        self.train_data.sampler.set_epoch(epoch)\n",
    "\n",
    "        x, y = get_batch(self.train_data)\n",
    "        x, y = x.to(self.gpu_id), y.to(self.gpu_id)\n",
    "        logits, loss = self.model(x, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            optimizer.step()\n",
    "\n",
    "    def _save_checkpoint(self, epoch):\n",
    "        ckp = self.model.module.state_dict()\n",
    "        PATH = \"checkpoint.pt\"\n",
    "        torch.save(ckp, PATH)\n",
    "        print(f\"Epoch {epoch} | Training checkpoint saved at {PATH}\")\n",
    "\n",
    "\n",
    "    def train(self, max_epochs):\n",
    "        for epoch in tqdm(range(max_epochs)):\n",
    "            self._run_epoch(epoch)\n",
    "            if self.gpu_id == 0 and epoch % self.save_every == 0:\n",
    "                self._save_checkpoint(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "533a5bfe-1de8-459c-9282-c91efc9e6a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.multiprocessing as mp\n",
    "# from torch.utils.data.distributed import DistributedSampler\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "# from torch.distributed import init_process_group, destroy_process_group\n",
    "# import os\n",
    "\n",
    "# def ddp_setup(rank, world_size):\n",
    "#     os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "#     os.environ[\"MASTER_PORT\"] = \"12345\"\n",
    "#     init_process_group(backend=\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "# def loader_setup():\n",
    "#     starts = torch.arange(0, len(train) - context_size - 1) \n",
    "#     sampler = DistributedSampler(starts, shuffle=True) \n",
    "#     def collate(starts_batch):\n",
    "#         xb = torch.stack([train[i:i+context_size] for i in starts_batch])\n",
    "#         yb = torch.stack([train[i+1:i+1+context_size] for i in starts_batch])\n",
    "#         return xb, yb\n",
    "    \n",
    "#     loader = DataLoader(starts, batch_size=batch_size, sampler=sampler, collate_fn=collate, drop_last=True)\n",
    "#     return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f090235-3d84-4cb7-b85a-5bf7c6ddffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 10000 \n",
    "# world_size = 4\n",
    "# save_every = 1000\n",
    "\n",
    "# def main(rank, world_size):\n",
    "#     model = BigramLanguageModel()\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "#     loader = loader_setup()\n",
    "#     ddp_setup(rank, world_size)\n",
    "    \n",
    "#     t = Trainer(model, loader, optimizer, rank, loader, save_every)\n",
    "#     t.train(epochs)\n",
    "    \n",
    "#     destroy_process_group()\n",
    "\n",
    "# mp.spawn(main, args=(world_size,), nprocs=world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "849717bd-51d9-4700-806d-5a86f57de579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rdu oue, ocJ! o Doru ahastbbosejin i tUaB Oi hX E sps otn oaouK n obLmn doa\n",
      "g:yc3eWCg HFu QNuesele sho\n",
      " aWv biqto oolhChn$TohFxmtGl haroaseaer:h  :OazoEu niVy;agSwroisdeoyGhntl-yrhs srecrgdJeelaOn;kR\n",
      "elFpine\n",
      "T!i pbyo:D thnbhe Mr my Euci\n",
      "Hd tizimheb ayWarreedadne G,e qlbdrewaon ja  ly a tlcCWt ,i?f\n",
      " \n",
      "oftwuysthOroQzK\n",
      "t m r'l, p,\n",
      "G\n",
      "un\n",
      "Koy er :zoI  n tt threUau on\n",
      ", an to srlCQ tnCdoioQo\n",
      " rrtaoslnoevRninite snlvqtheurier3t bu s'slEGawt3EK d s nlrereo t une nd  sqmros lahTs aAwmo niqXlr:y a,d a w,cKmupIkIyi insge;\n",
      "eg   tsaj'arr Etu t c m\n",
      "U srs\n",
      "ndilsoQatsqsp!r  smun\n",
      "g \n",
      "n$ em jtQ'irssPf, mtnbnnt meus tie  Iht so!  Usgslr fethzvsQ,ftii snvdroniViXt\n",
      "k\n",
      "!IeNNd te\n",
      "Rt iFe nvtt\n",
      "itMuy hNfkK keos;innow  syet,\n",
      " leersiIs  tn Rts ZCon\n",
      "li e\n",
      "er VCn\n",
      "\n",
      "ZoeQp\n",
      "T g cseiF\n",
      "\n",
      " ro hell:dvufI s Lh!ey ar\n",
      "e:\n",
      "o sn dhenohn;Pe VNd o\n",
      "oth'dir nBuJus\n",
      "RdZ ta3iwiemy s t! ree rtO3Gn a rheou O: rwe rqore jk\n",
      "a!Qo.r\n",
      "eo, v gur\n",
      "ey,n:\n",
      "ll s svcw'lSwltordBlzu,IcadowVI le rhylqlcarai'\n",
      "\n",
      "d\n",
      "ve?oiullt !.wer tk so .eit\n",
      "ssGe svool  s nt f ct,\n"
     ]
    }
   ],
   "source": [
    "print(decoder(t.model.generator(1000, 1).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d072e2cc-a110-4d76-8404-40e197b86da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"ckpt/model.0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae18bd86-1332-489a-82bf-6281439b82db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()\n",
    "\n",
    "model.load_state_dict(state)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f63f892-b047-4462-aea4-73fe8cce2e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Save that against the tigers putts;\n",
      "'Tis most revel done amazedness of evil.\n",
      "Yet, my lord, I spake my warber-lasts.\n",
      "\n",
      "Lord Mars.\n",
      "\n",
      "CAPULET:\n",
      "Who fly and apot, O PETESETER:\n",
      "Present here the sun swords are all unsub,\n",
      "Sin Romeo like that seat doth blush this page\n",
      "Shouts shall bsturn to count mine and wixtones\n",
      "Ere much the way ash to part hold ax himself.\n",
      "I have the hour'd and childly selveral place.\n",
      "Did I expect me himory to his course,\n",
      "And give him what we are not irefully sleep.\n",
      "\n",
      "SIR STENHOP:\n",
      "'Tis well, very well; before presenquating wrongs:\n",
      "And with her Ratcliff, her that revenged is,\n",
      "Enforch Planet and the duke's hell us: for me\n",
      "Uncleful banquit is dead, soft some all through for\n",
      "the precious of the lawful, he same\n",
      "induced affectious, to win the would with wearing's peace,\n",
      "Like to abunder hurried his all.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Well, let you must knock me not very told one,\n",
      "reters, take on twested split of.\n",
      "\n",
      "Nurse:\n",
      "Hark, you will fiery you so? how my captain\n",
      "Is the devils, as if he dure neet\n"
     ]
    }
   ],
   "source": [
    "print(decoder(model.generator(1000, 1).tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f836f29e-9b7f-4525-a42a-481b13f9d117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788929"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters()) # 10m params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f8cfae-4554-4104-9acf-5f93985b3ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base.3.11",
   "language": "python",
   "name": "base.3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
